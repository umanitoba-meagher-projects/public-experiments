{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIUkqBt8xFmy"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/umanitoba-meagher-projects/public-experiments/blob/main/jupyter-notebooks/Object%20Classification%20and%20Localization/dataset-sizes-classification-report.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRwyBsZYmbGz"
      },
      "outputs": [],
      "source": [
        "\n",
        "\"\"\"\n",
        "Author: Zhenggang Li & A.V. Ronquillo\n",
        "Date: May 21, 2024\n",
        "\n",
        "## Note: Note: The author generated this text in part with GPT-4,\n",
        "OpenAI\u2019s large-scale language-generation model. Upon generating\n",
        "draft code, the author reviewed, edited, and revised the code\n",
        "to their own liking and takes ultimate responsibility for\n",
        "the content of this code.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook introduces methods for evaluating the performance of a machine learning model in identifying and locating different animal species in trail camera photos. A classification model is used to classify and locate animal species across datasets of varying sizes (100, 500, and 1000 images). The notebook employs Python-based libraries such as `fastai`, `matplotlib`, and `sklearn` to preprocess data, train a convolutional neural network (CNN) using the `resnet34` architecture, and generate classification reports. These reports include precision, recall, F1-score, and support metrics for each class, visualized through bar plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Critical Uses & Adaptability\n",
        "\n",
        "## What the Notebook Can Be Used For:\n",
        "\n",
        "- **Dataset Exploration:**  \n",
        "  This notebook allows users to explore datasets of varying sizes, providing insights into how dataset size impacts model performance. It evaluates classification accuracy and localization metrics, offering a detailed understanding of the dataset's characteristics.\n",
        "\n",
        "- **Educational Purposes & Demonstrations:**  \n",
        "  The notebook serves as an educational resource for understanding Python-based machine learning workflows. It demonstrates the use of libraries like `fastai` and `sklearn` for image classification tasks, making it suitable for teaching concepts such as CNNs, data preprocessing, and performance evaluation.\n",
        "\n",
        "- **Feature Extraction:**  \n",
        "  By fine-tuning the `resnet34` model, the notebook extracts meaningful features from images, which can be used for further analysis or integrated into other machine learning pipelines.\n",
        "\n",
        "## How the Notebook Can Be Adapted:\n",
        "\n",
        "- **Integration with Spatial Design & Architectural Studies:**  \n",
        "  The notebook can be adapted for site analysis by replacing the animal image dataset with datasets containing architectural elements or spatial layouts. This enables the classification of design features or spatial patterns.\n",
        "\n",
        "- **Variables & Customization:**  \n",
        "  - The `species` and `dataset_size` variables in the dataset processing cells can be modified to accommodate different classes or dataset sizes.  \n",
        "  - The `batch_tfms` and `item_tfms` parameters in the `ImageDataLoaders` initialization allow customization of image transformations.\n",
        "\n",
        "- **Swapping Datasets:**  \n",
        "  - The dataset path in the `base_path` variable (e.g., in the cell processing the 100 dataset) can be updated to point to a custom dataset. This enables the notebook to work with entirely different image collections.\n",
        "\n",
        "- **Scalability:**  \n",
        "  The notebook can be scaled to handle larger datasets by adjusting the `batch_size` and `num_workers` parameters in the data loader. Additionally, the number of epochs in the `fine_tune` method can be increased for more extensive training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4mcmr4Ojx-s"
      },
      "source": [
        "# Module: Importing Additional Necessary Python Packages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZGxPqj7pHqE"
      },
      "source": [
        "For the purposes of the notebook in CoLab and the consideration of the runtime, the classification report will only be visualized through the 100, 500, and 1000 dataset for further analysis of the model's animal classification accuracy.\n",
        "\n",
        "The `matplotlib.pyplot` is assigned the alias of `plt`. This imported module is a library used for creating two-dimensional plots in Python. `sklearn.metrics` is used for visualization and model evaluation metrics. It essentially imports the `classification_report` function from the module. This element is another method of visualizing classification and localization accuracy in various quantitative metrics, allowing detailed model performance analysis. In doing so, it can provide insights that a confusion matrix may not necessarily offer. A classification report evaluates the performance of a classification model by calculating precision, recall, F1-score, and support for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1g2ljfmjwyt"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Borealis API configuration\nimport requests\nimport zipfile\n\nBOREALIS_SERVER = \"https://borealisdata.ca\"\n\ndef get_public_dataset_info(persistent_id):\n    \"\"\"\n    Get information about a public dataset\n    \"\"\"\n    url = f\"{BOREALIS_SERVER}/api/datasets/:persistentId/\"\n    params = {\"persistentId\": persistent_id}\n\n    response = requests.get(url, params=params)\n\n    if response.status_code == 200:\n        dataset_info = response.json()\n    else:\n        print(f\"Cannot access dataset: {response.status_code}\")\n        return None\n    \"\"\"\n    Get a list of files in a public dataset\n    \"\"\"\n    # Access the list of files from the dataset_info dictionary\n    files_list = dataset_info['data']['latestVersion']['files']\n\n    # Create an empty list to store file information\n    file_info_list = []\n\n    # Iterate through the files list and append file ID and filename to the list\n    for file_info in files_list:\n        file_id = file_info['dataFile']['id']\n        filename = file_info['dataFile']['filename']\n        file_info_list.append({\"file_id\": file_id, \"filename\": filename})\n\n    return file_info_list\n\ndef download_public_file(file_id, save_path=\"./\"):\n    \"\"\"\n    Download a specific public file from a dataset by its file ID\n    No authentication required\n    \"\"\"\n    url = f\"{BOREALIS_SERVER}/api/access/datafile/{file_id}\"\n\n    response = requests.get(url, stream=True)\n\n    if response.status_code == 200:\n        # Determine filename from headers or URL\n        filename = None\n        if \"Content-Disposition\" in response.headers:\n            cd = response.headers[\"Content-Disposition\"]\n            # Try to extract filename from content disposition\n            if \"filename=\" in cd:\n                filename = cd.split(\"filename=\")[1].strip('\"')\n\n        # Fallback to extracting from URL if header not available or malformed\n        if not filename:\n             filename = url.split(\"/\")[-1]\n\n        file_path = f\"{save_path}/{filename}\"\n\n        with open(file_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n\n        print(f\"SUCCESS: File downloaded to {file_path}\")\n        return file_path\n    else:\n        print(f\"ERROR: {response.status_code}: File may be restricted or not found\")\n        return None\n\ndef is_zip_file(filepath):\n    \"\"\"\n    Checks if a file is a valid zip file.\n    \"\"\"\n    return zipfile.is_zipfile(filepath)\n\ndef unzip_file(filepath, extract_path=\"./\"):\n    \"\"\"\n    Unzips a zip file to a specified path and returns the name of the top-level extracted folder.\n    Returns None if not a zip file or extraction fails.\n    \"\"\"\n    if is_zip_file(filepath):\n        try:\n            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n                # Get the name of the top-level directory within the zip\n                # Assumes there is a single top-level directory\n                top_level_folder = None\n                for file_info in zip_ref.infolist():\n                    parts = file_info.filename.split('/')\n                    if parts[0] and len(parts) > 1:\n                        top_level_folder = parts[0]\n                        break # Assuming the first entry gives the top-level folder\n\n                zip_ref.extractall(extract_path)\n                print(f\"SUCCESS: Successfully unzipped {filepath} to {extract_path}\")\n                return top_level_folder\n\n        except Exception as e:\n            print(f\"ERROR: Error unzipping {filepath}: {e}\")\n            return None\n    else:\n        print(f\"INFO: {filepath} is not a valid zip file.\")\n        return None\n\n# Initialize Borealis dataset access\npublic_doi = \"doi:10.5683/SP3/H3HGWF\"\nprint(\"Borealis dataset initialized for animal notebook data.\")",
        "from fastai.vision.all import *\n",
        "from pathlib import Path\n",
        "\n",
        "#Additional Packages\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Sd2sSCq-qW9"
      },
      "source": [
        "##### Google Drive File Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smYVIlbB-jg9"
      },
      "outputs": [],
      "source": [
        "def mount_google_drive():\n",
        "# Borealis API configuration\nimport requests\nimport zipfile\n\nBOREALIS_SERVER = \"https://borealisdata.ca\"\n\ndef get_public_dataset_info(persistent_id):\n    \"\"\"\n    Get information about a public dataset\n    \"\"\"\n    url = f\"{BOREALIS_SERVER}/api/datasets/:persistentId/\"\n    params = {\"persistentId\": persistent_id}\n\n    response = requests.get(url, params=params)\n\n    if response.status_code == 200:\n        dataset_info = response.json()\n    else:\n        print(f\"Cannot access dataset: {response.status_code}\")\n        return None\n    \"\"\"\n    Get a list of files in a public dataset\n    \"\"\"\n    # Access the list of files from the dataset_info dictionary\n    files_list = dataset_info['data']['latestVersion']['files']\n\n    # Create an empty list to store file information\n    file_info_list = []\n\n    # Iterate through the files list and append file ID and filename to the list\n    for file_info in files_list:\n        file_id = file_info['dataFile']['id']\n        filename = file_info['dataFile']['filename']\n        file_info_list.append({\"file_id\": file_id, \"filename\": filename})\n\n    return file_info_list\n\ndef download_public_file(file_id, save_path=\"./\"):\n    \"\"\"\n    Download a specific public file from a dataset by its file ID\n    No authentication required\n    \"\"\"\n    url = f\"{BOREALIS_SERVER}/api/access/datafile/{file_id}\"\n\n    response = requests.get(url, stream=True)\n\n    if response.status_code == 200:\n        # Determine filename from headers or URL\n        filename = None\n        if \"Content-Disposition\" in response.headers:\n            cd = response.headers[\"Content-Disposition\"]\n            # Try to extract filename from content disposition\n            if \"filename=\" in cd:\n                filename = cd.split(\"filename=\")[1].strip('\"')\n\n        # Fallback to extracting from URL if header not available or malformed\n        if not filename:\n             filename = url.split(\"/\")[-1]\n\n        file_path = f\"{save_path}/{filename}\"\n\n        with open(file_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n\n        print(f\"SUCCESS: File downloaded to {file_path}\")\n        return file_path\n    else:\n        print(f\"ERROR: {response.status_code}: File may be restricted or not found\")\n        return None\n\ndef is_zip_file(filepath):\n    \"\"\"\n    Checks if a file is a valid zip file.\n    \"\"\"\n    return zipfile.is_zipfile(filepath)\n\ndef unzip_file(filepath, extract_path=\"./\"):\n    \"\"\"\n    Unzips a zip file to a specified path and returns the name of the top-level extracted folder.\n    Returns None if not a zip file or extraction fails.\n    \"\"\"\n    if is_zip_file(filepath):\n        try:\n            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n                # Get the name of the top-level directory within the zip\n                # Assumes there is a single top-level directory\n                top_level_folder = None\n                for file_info in zip_ref.infolist():\n                    parts = file_info.filename.split('/')\n                    if parts[0] and len(parts) > 1:\n                        top_level_folder = parts[0]\n                        break # Assuming the first entry gives the top-level folder\n\n                zip_ref.extractall(extract_path)\n                print(f\"SUCCESS: Successfully unzipped {filepath} to {extract_path}\")\n                return top_level_folder\n\n        except Exception as e:\n            print(f\"ERROR: Error unzipping {filepath}: {e}\")\n            return None\n    else:\n        print(f\"INFO: {filepath} is not a valid zip file.\")\n        return None\n\n# Initialize Borealis dataset access\npublic_doi = \"doi:10.5683/SP3/H3HGWF\"\nprint(\"Borealis dataset initialized for animal notebook data.\")",
        "    print(\"Google Drive is mounted. Proceed with file operations.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvKZL5vlwzIl"
      },
      "source": [
        "# Module: Repeating Error Handling Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oERmE6dPxqo0"
      },
      "source": [
        "The same snippet of code from the script for the Confusion Matrix will be utilized for the functions that focuses on error handling in image files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC5OmJvZxVjd"
      },
      "outputs": [],
      "source": [
        "def safe_get_image_files(path):\n",
        "    image_files = []\n",
        "    for img_path in get_image_files(path):\n",
        "        try:\n",
        "            # Attempt to open the image to verify it is not corrupted\n",
        "            with PILImage.create(img_path) as img:\n",
        "                image_files.append(img_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping file {img_path} due to error: {e}\")\n",
        "    return image_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8860XOOzAaO"
      },
      "source": [
        "# Module: Data Processing & Fine-Tuning the Model\n",
        "\n",
        "In this module, the same lines of code from the Confusion Matrix script of gathering images based on `species` and `size` is also repeated.\n",
        "\n",
        "The `cnn_learner` is still using the `resnet34` architecture as well as fastai, and it ensures that the model focuses on using `accuracy` as the model's metric for image classification and localization accuracy. The model is then fine-tuned for `4` epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uodbs0aRye_O"
      },
      "outputs": [],
      "source": [
        "def process_dataset(base_path, species, size):\n",
        "    files = []\n",
        "    # Gather files for all species for the given size\n",
        "    for animal in species:\n",
        "        path = Path(base_path) / f'{animal}_{size}'\n",
        "        files += safe_get_image_files(path)\n",
        "\n",
        "    if not files:\n",
        "        print(f\"No images found for dataset size {size}. Skipping...\")\n",
        "        return\n",
        "\n",
        "    # Initialize Data Loaders\n",
        "    dls = ImageDataLoaders.from_path_func(\n",
        "        path=base_path,\n",
        "        fnames=files,\n",
        "        label_func=lambda x: x.parent.name.split('_')[0],\n",
        "        item_tfms=Resize(460),\n",
        "        batch_tfms=aug_transforms(size=224),\n",
        "        bs=32,\n",
        "        num_workers=0,\n",
        "        valid_pct=0.8  # ensure there is a validation set\n",
        "    )\n",
        "\n",
        "    # Initialize CNN Learner and Fine-Tune the Model\n",
        "    learn = cnn_learner(dls, resnet34, metrics=accuracy)\n",
        "    learn.fine_tune(4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcsZLSj5ymgT"
      },
      "source": [
        "# Module: Retrieving Predictions & True Labels\n",
        "After the training, the model's predictions and true labels are retrieved. By using a container tuple, the `get_preds` method gathers the predicted probabilities and the true labels. The raw predictions outputs are converted to probabilities using the `preds_softmax` function that sum to 1 for each sample.\n",
        "\n",
        "The `max` function is applied along the first dimension where (axis=1), which corresponds to the different classes. The `predicted_probs` variable contains the maximum probabilities, and the `actual_classes` variable contains the predicted class labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdQ1vVajyUSs"
      },
      "outputs": [],
      "source": [
        "    preds, y_true = learn.get_preds()\n",
        "    preds_softmax = preds.softmax(dim=1)\n",
        "    predicted_probs, actual_classes = preds_softmax.max(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQiEE4nN8UvN"
      },
      "source": [
        "# Module: Converting the Report into a Pandas DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hurfgbj9tqW"
      },
      "source": [
        "By using the `argmax` function, the `y_pred` calculates the predicted class labels by taking the maximum value along the first dimension `(dim-1)` of the `preds_softmax` tensor. This establishes that the `preds_softmax` contains the predicted probabilities for each class.\n",
        "\n",
        "The `report` function generates the `classification_report` from the `sklearn.metrics` module that was previously imported. It takes the true labels `y_true` and the predicted class labels `y_pred` as input, and sets `output_dict=True` to return the report as a dictionary. In doing so, it can allow access to the individual metrics for each class, including the overall metrics in a more structured manner.\n",
        "\n",
        "Transposing the `DataFrame` swaps the rows and columns so that class labels become the index and the metrics (precision, recall, f1-score, etc.) become the columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXTxnmlH8VHF"
      },
      "outputs": [],
      "source": [
        "    # Assuming y_true and preds_softmax are your true labels and output probabilities.\n",
        "    y_pred = preds_softmax.argmax(dim=1)\n",
        "    report = classification_report(y_true, y_pred, output_dict=True)\n",
        "    df = pd.DataFrame(report).transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmT_BF03JTUG"
      },
      "source": [
        "# Module: Keys Dictionary & Generating the Classification Visual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PNEpVcTJVel"
      },
      "source": [
        "The dictionary for the report contains these keys:\n",
        "\n",
        "  `accuracy`: Overall classification accuracy of the model. The percentage of correctly predicted instances out of all instances in the dataset.\n",
        "\n",
        "  `macro avg`: The macro-averaged precision, recall, f1-score, and support across all classes. This calculates the metric for each class and then takes the average across all classes. This gives equal weight to each class, regardless of the number of instances in each class.\n",
        "\n",
        "  `weighted avg`: Weighted average of the precision, recall, f1-score, and support across all classes. Weighted averaging calculates each class metric by the number of instances in that class, giving more weight to larger classes.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  Class-specific metrics: For each class, the dictionary contains the following metrics:\n",
        "\n",
        "  `precision`: The percentage of predicted positive instances that are actually positive. It is calculated as the number of true positives divided by the total number of predicted positives.\n",
        "\n",
        "  `recall`: The percentage of actual positive instances that are predicted to be positive. It is calculated as the number of true positives divided by the total number of actual positives.\n",
        "\n",
        "  `f1-score`: The harmonic mean of precision and recall. It is calculated as 2 * (precision * recall) / (precision + recall).\n",
        "\n",
        "  `support`: Number of true instances for the class\n",
        "\n",
        "\n",
        "  These metrics can provide a sharper control of how you want to evaluate the model's performance on each class individually. Therefore it can be useful for identifying classes that the model is struggling with or for comparing the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7F54wMseJUxk"
      },
      "outputs": [],
      "source": [
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(data=df, x=df.index, y='precision', label='Precision', color='b')\n",
        "    sns.barplot(data=df, x=df.index, y='recall', label='Recall', color='r', alpha=0.6)\n",
        "    sns.barplot(data=df, x=df.index, y='f1-score', label='F1-Score', color='g', alpha=0.3)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.xlabel('Classes')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.title(f'Classification Report for {size} Dataset')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJaJVdZUSi1Y"
      },
      "source": [
        "`figsize` ensures that the specified size of the visual is 10 inches wide and 6 inches high. By using the Seaborn library, the `sns.barplot` creates a bar graph visual. The `label` argument specifies the label for the plot, and the `color` argument specifies the color of the bars. The `alpha` arguement controls the transparency of the `recall` bar as well as the `f1-score`. These parameters can be controlled to your liking in terms of what you want to communicate through the classification report."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YhduQtxcd5p"
      },
      "source": [
        "# Module: Process Datasets to create the Classification Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uav1YV4tcnz5"
      },
      "source": [
        "Similar to initializing the Confusion Matrix generation, this module uses the same snippet of code to process each dataset size for the Classification Report. In doing so, it ensures that necessary files and dataset sizes from Google Drive are made accessible for processing. This first run of the script is the  Classification Report for the 100 Dataset, the same steps are repated for each dataset in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDNsp70MeCqJ"
      },
      "source": [
        "# 100 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMZwX4hPefI5"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    mount_google_drive()\n",
        "    base_path = './av datasets'\n",
        "    species = ['fox', 'squirrel', 'deer']\n",
        "    dataset_size = 100\n",
        "    process_dataset(base_path, species, dataset_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nOoJujt7zXA"
      },
      "source": [
        "# 500 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEYMmk42pvwM"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    mount_google_drive()\n",
        "    base_path = './av datasets'\n",
        "    species = ['fox', 'squirrel', 'deer']\n",
        "    dataset_size = 500\n",
        "    process_dataset(base_path, species, dataset_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1000 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pTGINuT1p0RE"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    mount_google_drive()\n",
        "    base_path = './av datasets'\n",
        "    species = ['fox', 'squirrel', 'deer']\n",
        "    dataset_size = 1000\n",
        "    process_dataset(base_path, species, dataset_size)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}