{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1884d2b1",
      "metadata": {},
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/umanitoba-meagher-projects/public-experiments/blob/main/jupyter-notebooks/Object%20Classification%20and%20Localization/bounding-boxes-coco-weights.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0063121a-197c-4b0d-8e55-5159458e5fbe",
      "metadata": {
        "id": "0063121a-197c-4b0d-8e55-5159458e5fbe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Author: Mitch Constable & A.V. Ronquillo\n",
        "Date: May 17, 2024\n",
        "\n",
        "Purpose: To create bounding boxes using resnet50 on dataset\n",
        "of animal images.\n",
        "\n",
        "Note: The author generated this text in part with GPT-4,\n",
        "OpenAI\u2019s large-scale language-generation model. Upon generating\n",
        "draft code, the author reviewed, edited, and revised the code\n",
        "to their own liking and takes ultimate responsibility for\n",
        "the content of this code.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f089e5bb",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook uses a pre-trained Faster R-CNN model to automatically detect and locate objects in 100 deer images by generating bounding boxes with confidence scores. The workflow loads the model, processes images, runs inference, and filters results based on a configurable confidence threshold before saving annotated outputs. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea09d2b1",
      "metadata": {},
      "source": [
        "# Critical Uses & Adaptability\n",
        "\n",
        "## What the Notebook Can Be Used For:\n",
        "\n",
        "- **Dataset Exploration:** The notebook enables exploration of image datasets by automatically detecting and highlighting objects of interest. This is useful for quickly assessing the content and quality of a dataset, identifying labeling inconsistencies, or verifying the presence of target objects (in our case urban animals).\n",
        "\n",
        "- **Educational Purposes & Demonstrations:** The notebook shows how Python code, scripting, and machine learning models can be applied to image-based tasks. By extracting bounding box coordinates, class labels, and confidence scores, the structured data can be used for further analysis such as statistical summaries, dataset curation, or as input features for other machine learning models.\n",
        "\n",
        "## How the Notebook Can Be Adapted:\n",
        "\n",
        "- **Integration with Spatial Design:** The methodology can be extended to site analysis or spatial design projects by applying object detection to images of built environments, landscapes, or architectural sites. Detected features can inform spatial mapping, usage studies, or environmental assessments.\n",
        "\n",
        "- **Variables & Customization:** Variables like `confidence_threshold` (see Cell 7) and `image_dir` (see Cell 6) can be adjusted to refine detection sensitivity or to point to different image sources. By changing the `image_dir` variable in the code block in Cell 6 you can process any directory of images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f575c9c3-d2af-4c75-8eef-7f69c54ac90a",
      "metadata": {
        "id": "f575c9c3-d2af-4c75-8eef-7f69c54ac90a"
      },
      "source": [
        "# Module: Importing Essential Libraries\n",
        "These lines import necessary modules. `torch` is the main PyTorch library. The `fastercnn_resnet50_fpn` function loads the model architecture used for object detection. Image operations are handled by `PIL` and `torchvision` torchvision, while `glob` helps in file navigation, and `matplotlib` is used for plotting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aac5ae7f-251b-4d33-b207-9bc99e9b7485",
      "metadata": {
        "id": "aac5ae7f-251b-4d33-b207-9bc99e9b7485"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.transforms import functional as F\n",
        "from PIL import Image\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zHbwLe1IBMlI",
      "metadata": {
        "id": "zHbwLe1IBMlI"
      },
      "source": [
        "# Module: Set up the Object Detection Model & Image Directory\n",
        "\n",
        "The script loads a pre-trained Faster R-CNN model and sets it to evaluation mode, disabling certain operations like Dropout. Furthermore, after preparing the `F.to_tensor`, it sets up the transformation to convert images to PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc9606bc-8d5d-4f3f-9cde-2decfae5e235",
      "metadata": {
        "id": "bc9606bc-8d5d-4f3f-9cde-2decfae5e235"
      },
      "outputs": [],
      "source": [
        "# Load the pretrained model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "model = model.eval()\n",
        "\n",
        "# Prepare the image transform\n",
        "transform = F.to_tensor\n",
        "\n",
        "# Set your image directory path\n",
        "# Borealis API configuration\nimport requests\nimport zipfile\n\nBOREALIS_SERVER = \"https://borealisdata.ca\"\n\ndef get_public_dataset_info(persistent_id):\n    \"\"\"\n    Get information about a public dataset\n    \"\"\"\n    url = f\"{BOREALIS_SERVER}/api/datasets/:persistentId/\"\n    params = {\"persistentId\": persistent_id}\n\n    response = requests.get(url, params=params)\n\n    if response.status_code == 200:\n        dataset_info = response.json()\n    else:\n        print(f\"Cannot access dataset: {response.status_code}\")\n        return None\n    \"\"\"\n    Get a list of files in a public dataset\n    \"\"\"\n    # Access the list of files from the dataset_info dictionary\n    files_list = dataset_info['data']['latestVersion']['files']\n\n    # Create an empty list to store file information\n    file_info_list = []\n\n    # Iterate through the files list and append file ID and filename to the list\n    for file_info in files_list:\n        file_id = file_info['dataFile']['id']\n        filename = file_info['dataFile']['filename']\n        file_info_list.append({\"file_id\": file_id, \"filename\": filename})\n\n    return file_info_list\n\ndef download_public_file(file_id, save_path=\"./\"):\n    \"\"\"\n    Download a specific public file from a dataset by its file ID\n    No authentication required\n    \"\"\"\n    url = f\"{BOREALIS_SERVER}/api/access/datafile/{file_id}\"\n\n    response = requests.get(url, stream=True)\n\n    if response.status_code == 200:\n        # Determine filename from headers or URL\n        filename = None\n        if \"Content-Disposition\" in response.headers:\n            cd = response.headers[\"Content-Disposition\"]\n            # Try to extract filename from content disposition\n            if \"filename=\" in cd:\n                filename = cd.split(\"filename=\")[1].strip('\"')\n\n        # Fallback to extracting from URL if header not available or malformed\n        if not filename:\n             filename = url.split(\"/\")[-1]\n\n        file_path = f\"{save_path}/{filename}\"\n\n        with open(file_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n\n        print(f\"SUCCESS: File downloaded to {file_path}\")\n        return file_path\n    else:\n        print(f\"ERROR: {response.status_code}: File may be restricted or not found\")\n        return None\n\ndef is_zip_file(filepath):\n    \"\"\"\n    Checks if a file is a valid zip file.\n    \"\"\"\n    return zipfile.is_zipfile(filepath)\n\ndef unzip_file(filepath, extract_path=\"./\"):\n    \"\"\"\n    Unzips a zip file to a specified path and returns the name of the top-level extracted folder.\n    Returns None if not a zip file or extraction fails.\n    \"\"\"\n    if is_zip_file(filepath):\n        try:\n            with zipfile.ZipFile(filepath, 'r') as zip_ref:\n                # Get the name of the top-level directory within the zip\n                # Assumes there is a single top-level directory\n                top_level_folder = None\n                for file_info in zip_ref.infolist():\n                    parts = file_info.filename.split('/')\n                    if parts[0] and len(parts) > 1:\n                        top_level_folder = parts[0]\n                        break # Assuming the first entry gives the top-level folder\n\n                zip_ref.extractall(extract_path)\n                print(f\"SUCCESS: Successfully unzipped {filepath} to {extract_path}\")\n                return top_level_folder\n\n        except Exception as e:\n            print(f\"ERROR: Error unzipping {filepath}: {e}\")\n            return None\n    else:\n        print(f\"INFO: {filepath} is not a valid zip file.\")\n        return None\n\n# Initialize Borealis dataset access\npublic_doi = \"doi:10.5683/SP3/H3HGWF\"\nprint(\"Borealis dataset initialized for animal notebook data.\")",
        "# File path on Google Drive\n",
        "file_path = './deer_100/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e879e120-3d5f-4b4a-a395-7dee0d5d4acf",
      "metadata": {
        "id": "e879e120-3d5f-4b4a-a395-7dee0d5d4acf"
      },
      "source": [
        "# Module: Threshold Detection Confidence\n",
        "The `confidence_threshold`is set to 0.5 for detection confidence. Only detections with a confidence score greater than or equal to 0.5 will be considered valid. In this case, 0.5 is the minimum score that detected objects must achieve to be considered valid detections. This score, typically between 0 and 1, represents the model's confidence in the accuracy of its prediction for each object it detects in an image. Detections below this confidence level are not processed further or displayed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc232572-5a1a-44b5-96c6-8a6fcf527702",
      "metadata": {
        "id": "fc232572-5a1a-44b5-96c6-8a6fcf527702"
      },
      "outputs": [],
      "source": [
        "# Set a threshold for detection confidence\n",
        "confidence_threshold = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f972e9a9-874e-4dce-8eae-7e24b15fc5da",
      "metadata": {
        "id": "f972e9a9-874e-4dce-8eae-7e24b15fc5da"
      },
      "source": [
        "# Module: Processing the Deer Images\n",
        "By using a `for` loop, the script iterates through each `.jpg` file in the specified directory, then the current image file opens and converts it to the RGB color space through the `Image.open(image_path).convert('RGB')` statement, allowing the images to be processed. The `image_tensor` applies a transformation to the image. This transformation could involve resizing, normalization, or other preprocessing steps required by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d137e3c9-12eb-4505-9660-aa661df7b8d4",
      "metadata": {
        "id": "d137e3c9-12eb-4505-9660-aa661df7b8d4"
      },
      "outputs": [],
      "source": [
        "for image_path in glob.glob(f'{file_path}/*.jpg'):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "745d109c-e4fa-4c8f-98cf-b66a4dae2e35",
      "metadata": {
        "id": "745d109c-e4fa-4c8f-98cf-b66a4dae2e35"
      },
      "source": [
        "# Module: Predicting using the Model without Gradient Calculation\n",
        "This context manager disables gradient computation using `torch.no_grad()` during the model's prediction. This is common practice during inference (prediction) to save computational resources like using less memory and speeds up the computation, then passing the tensor through the model. By making a `prediction`, the `model([image_tensor])` script uses the pre-trained model to make a prediction on the current image tensor. The model's output is stored in the prediction variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "032f7518-0d09-4627-b063-2138f8709982",
      "metadata": {
        "id": "032f7518-0d09-4627-b063-2138f8709982"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    prediction = model([image_tensor])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da9ee78d-3335-455e-bdd3-29160ae3a0da",
      "metadata": {
        "id": "da9ee78d-3335-455e-bdd3-29160ae3a0da"
      },
      "source": [
        "# Module: Extracting Prediction Data\n",
        "This module retrieves the detection results composed of the bounding box coordinates, detection confidence scores, and the categories' labels. The `boxes` extracts the bounding boxes of the detected objects from the model's prediction. The `scores` extracts the confidence scores associated with each detected object, while the `labels` extracts the object classes corresponding to each detected object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3adef5e-e167-4bf2-8249-463649dc6e11",
      "metadata": {
        "id": "c3adef5e-e167-4bf2-8249-463649dc6e11"
      },
      "outputs": [],
      "source": [
        "boxes = prediction[0]['boxes']\n",
        "scores = prediction[0]['scores']\n",
        "labels = prediction[0]['labels']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11664a2a-e622-4601-91af-60128c326fce",
      "metadata": {
        "id": "11664a2a-e622-4601-91af-60128c326fce"
      },
      "source": [
        "# Module: Plotting the Image for Visualization\n",
        "This module creates a plot for the image. A new figure and an axes object is created for displaying the image with detected objects.\n",
        "This `ax.imshow(image)` displays the current image on the axes object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45a81058-f638-46e3-879d-148ad75084eb",
      "metadata": {
        "id": "45a81058-f638-46e3-879d-148ad75084eb"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1)\n",
        "ax.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f50105c5-01a6-4b3a-90cb-3a2cb49e330b",
      "metadata": {
        "id": "f50105c5-01a6-4b3a-90cb-3a2cb49e330b"
      },
      "source": [
        "# Module: Filter by Confidence Score & Extract Bounding Boxes Coordinates\n",
        "This module iterates over each detection using the `for` loop statement. It checks if its score exceeds the threshold, prints the details, and plots a rectangle over the image if it does exceed the value threshold. The `boxes`, `scores`, and `labels` are zipped together so that elements from each list (all related to a single detection) are processed in unison within each iteration of the loop.\n",
        "\n",
        "A list of coordinates is unpacked for the bounding boxes identifying detected objects. Each element is a set of coordinates `x, y, xmax, ymax = box` indicating the rectangular regions of the image where objects are detected. The (x, y) is of the upper-left corner and (xmax, ymax) of the lower-right corner of the rectangle.\n",
        "\n",
        "The `if score` conditional statement checks if the object's confidence score is greater than or equal to the specified threshold. If the score meets the threshold, the `(f'Image: {image_path}, Label: {label.item()}, Score: {score.item()}')` line prints information about the detected object, including the image path, label, and score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d07fc66-5058-468b-9836-8dc242905719",
      "metadata": {
        "id": "3d07fc66-5058-468b-9836-8dc242905719"
      },
      "outputs": [],
      "source": [
        "for box, score, label in zip(boxes, scores, labels):\n",
        "    if score > confidence_threshold:\n",
        "        # Code to print details and draw rectangles\n",
        "        print(f'Image: {image_path}, Label: {label.item()}, Score: {score.item()}')\n",
        "        x, y, xmax, ymax = box"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98aaa4e0-25c3-4ca7-aeac-bf309277d837",
      "metadata": {
        "id": "98aaa4e0-25c3-4ca7-aeac-bf309277d837"
      },
      "source": [
        "# Module: Drawing the Rectangle Patch\n",
        "This code creates a rectangle patch (from matplotlib.patches) to visually represent the bounding box. This is executed by using the extracted coordinates. The rectangle has a red border and no fill colour. In this manner, `ax.add_patch(rect)` adds the rectangle object to the current axes object.\n",
        "\n",
        "By executing this loop, the script annotates each image in the dataset with red bounding boxes around detected objects considered reliably detected based on the confidence threshold, with associated information printed about each detection. This visual output is useful for verifying and demonstrating the performance of the object detection model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc892822-9f31-4de7-b30b-eb10aba16caa",
      "metadata": {
        "id": "fc892822-9f31-4de7-b30b-eb10aba16caa"
      },
      "outputs": [],
      "source": [
        "rect = patches.Rectangle((x, y), (xmax - x), (ymax - y), linewidth=1, edgecolor='r', facecolor='none')\n",
        "ax.add_patch(rect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c31ceaef-e45b-457d-8da3-e0f57b862f96",
      "metadata": {
        "id": "c31ceaef-e45b-457d-8da3-e0f57b862f96"
      },
      "source": [
        "# Module: Saving the Annotated Images\n",
        "This module constructs a `save_path` by prefixing the original `image_name` with the 'annotated_' prefix, it then saves the plot and image with the bounding box to this path location, and closes the plot to free up memory.\n",
        "\n",
        "Overall, this Python script is useful for applying an object detector to a set of images and visualizing the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edb3ef27-d9fc-4105-84ab-d190c1f88d7d",
      "metadata": {
        "id": "edb3ef27-d9fc-4105-84ab-d190c1f88d7d"
      },
      "outputs": [],
      "source": [
        "# Save the image with bounding boxes in the same folder\n",
        "image_name = os.path.basename(image_path)\n",
        "save_path = os.path.join(file_path, f'annotated_{image_name}')\n",
        "# Save with 'annotated_' prefix\n",
        "plt.savefig(save_path)\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5AM75-EhddVo",
      "metadata": {
        "id": "5AM75-EhddVo"
      },
      "source": [
        "After running the script in its entirety, it will take several minutes for the images to be annotated. The newly annotated images with the bounding boxes can be found in the original 'deer_100' folder from which the data set came from in the directory."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}